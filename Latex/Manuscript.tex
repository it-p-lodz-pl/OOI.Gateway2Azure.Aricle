\documentclass{article}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{stfloats}
\usepackage{url}
\usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{txfonts}
\usepackage[textsize=tiny]{todonotes}

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{example}[definition]{Example}


\providecommand{\tightlist} { \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\providecommand{\keywords}[1]{ \small \textbf{\textit{Keywords---}} #1 }

\title{Object-Oriented Internet - Azure Interoperability}
%\\headtitle{OOI - Azure Interoperability}

\author[1]{Mariusz Postół, Piotr Szymczak}
%\author[2]{Piotr Szymczak}
%\headauthor{Postół M, Szymczak P}
\affil[1]
{
 Lodz University of Technology\\
 Institute of Information Technology\\
 ul. Wólczańska 215, 90-924 Lodz, Poland\\
 mailto:mariusz.postol@p.lodz.pl
}
\date{}

\begin{document}

\maketitle

\begin{abstract}

      Optimization of the industrial processes requires further research on the integration of machine-centric systems with human-centric cloud-based services in the context of new emerging disciplines, namely Industry 4.0 and Industrial Internet of Things. This research aims at working out a new generic architecture and deployment scenario applicable to this integration. The reactive interoperability relationship of the interconnected nodes is proposed to deal with the network traffic propagation asymmetry or assets' mobility. Described solution based on the OPC Unified Architecture international standard relaxes issues related to the real-time multi-vendor environment. The discussion addressing the generic architecture concludes that the embedded gateway archetype best suits all requirements. To promote separation of concerns and reusability, the proposed architecture of the embedded gateway archetype has been implemented as a composable part of a selected OPC UA PubSub framework.

      The proposals are backed by proof of concept reference implementations confirming the possibility to integrate selected cloud services with the cyber-physical system interconnected as one whole atop of the OPC UA by applying the proposed architecture and deployment scenario. It is contrary to interconnecting cloud services with a selected OPC UA server limiting the PubSub role to data export only.

\end{abstract}

\keywords{ Azure, Cloud Computing, Object-Oriented Internet, Industrial communication, Industry 4.0, Internet of Things, Machine to Machine Communication, OPC Unified Architecture,
Reactive Networking (RxNetworking)  }

\maketitle

%\listoftodos[TODO list]

\section{Introduction}\label{introduction}

All the time, the Information and Communication Technology is providing society with a vast variety of new distributed applications aimed at micro and macro optimization of the industrial processes. Obviously, the design foundation of this kind of application has to focus primarily on communication technologies. Based on the role humans take while using these applications they can be grouped as follows:

\begin{itemize}
      \item \textbf{human-centric} - information origin or ultimate information destination is an operator,
      \item \textbf{machine-centric} - information creation, consumption, networking, and processing are achieved entirely without human interaction.
\end{itemize}

A typical \textbf{human-centric} approach is a web-service supporting, for example, a web user interface (UI) to monitor conditions and manage millions of devices and their data in a typical cloud-based IoT approach. In this case, it is characteristic that any uncertainty and necessity to make a decision can be relaxed by human interaction. Coordination of robots behavior in a work-cell (automation islands) is a \textbf{machine-centric} example. In this case, any human interaction must be recognized as impractical or even impossible. This interconnection scenario requires the machine to machine communication (M2M) demanding multi-vendor devices integration.

From the M2M communication concept, a broader idea of a smart factory can be derived. In this M2M  deployment approach, the mentioned robots are only executive assets of an integrated supervisory control system responsible for macro optimization of an industrial process composed as one whole. Deployment of the smart factory concept requires a hybrid solution and interconnection of the mentioned above heterogeneous environments. This approach is called the fourth industrial revolution and was coined as Industry 4.0. It is worth stressing that machines - or more general assets - interconnection is not enough, and additionally, assets interoperability has to be expected for the deployment of this concept. In this case, multi-vendor integration makes communication standardization especially important, namely, it is required that the payload of the message is standardized to be factored on the data-gathering site and consumed on the ultimate destination site.

Highly-distributed solutions used to control real-time process aggregating islands of automation (e.g.~virtual power plants producing renewable energy) additionally must leverage public communication infrastructure, namely the Internet. Internet is a demanding environment for highly distributed process control applications designed atop the M2M communication paradigm because

\begin{itemize}
      \item it is a globally shareable and can be also used by malicious users
      \item it offers only non-deterministic communication making integration of islands of automation designed against the real-time requirements a difficult task
\end{itemize}

Today both obstacles can be overcome, and as examples, we have bank account remote control and voice over IP in daily use. The first application must be fine-tuned in the context of data security, and the second is very sensitive in regard to time constraints. Similar approaches could be applied to adopt the well known in process control industry concepts:

\begin{itemize}
      \item  Human Machine Interface (HMI)
      \item Supervisory Control and Data Acquisition (SCADA)
      \item Distributed Control Systems (DCS)
\end{itemize}

A detailed examination of these solutions is far beyond the scope of this article. It is only worth stressing that, by design, all of them are designed on the foundation of interactive communication. Interactive communication is based on a data polling foundation. In this case, the application must follow the interactive behavioral model, because it actively polls the data source for more information by pulling data from a sequence that represents the process state in time. The application is active in the data retrieval process - it controls the pace of the retrieval by sending the requests at its convenience. Such a polling pattern is similar to visiting the books shop and checking out a book. After you are done with the book, you pay another visit to check out another one. If the book is not available you must wait, but you may read what you selected. The client/server archetype is well suited for the mentioned above applications.

After dynamically attaching a new island of automation the control application (responsible for the data pulling) must be reconfigured for this interoperability scenario. In other words, in this case, the interactive communication relationship cannot be directly applied because the control application must be informed on how to pull data from a new source. As a result, a plug and produce scenario \cite{PlugProduceByModellingSkills} cannot be seamlessly applied. A similar drawback must be overcome if for security reasons suitable protection methods have been applied to make network traffic propagation asymmetric. It is accomplished using intermediary devices, for example, firewalls, to enforce traffic selective availability based on predetermined security rules against unauthorized access.

Going further, we shall assume that islands of automation are mobile, e.g.~autonomous cars passing a supervisory controlled service area. In this case, the behavior of the interconnected assets is particularly important concerning the environment in which they must interact. This way we have entered the Internet of Things domain of Internet-based applications.

If we must bother with the network traffic propagation asymmetry or mobility of the asset network attachment-points the reactive relationship could relax the problems encountered while the interactive approach is applied. In this case, the sessionless publisher-subscriber communication relationship is a typical pattern to implement the abstract reactive interoperability paradigm. The sessionless relationship is a message distribution scenario where senders of messages, called publishers, do not send them directly to specific receivers, called subscribers, but instead, categorize the published messages into topics without knowledge about which subscribers if any, there may be. Similarly, subscribers express interest in one or more topics and only receive messages that are of interest, without knowledge about which publishers, if any, there are. In this scenario, the publishers and subscribers are loosely coupled i.e they are decoupled in time, space and synchronization \cite{RefWorks:doc:5c44e246e4b0591b15ea9e59}.

If the \textbf{machine-centric} interoperability - making up islands of automation - must be monitored and/or controlled by a supervisory system cloud computing concept may be recognized as a beneficial solution to replace or expand the mentioned above applications, i.e.~HMI, SCADA, DCS, etc. Cloud computing is a method to provide a requested functionality as a set of services. There are many examples that cloud computing is useful to reduce costs and increase robustness. It is also valuable in case the process data must be exposed to many stakeholders. Following this idea and offering control systems as a service, there is required a mechanism created on the service concept and supporting abstraction and virtualization - two main pillars of the cloud computing paradigm. In the cloud computing concept, virtualization is recognized as the possibility to share the services by many users, and abstraction hides implementation details.

Deployment of the hybrid solution providing interoperability of the \textbf{machine-centric} Cyber-Physical System (CPS) and \textbf{human-centric} cloud-based front-end can be implemented applying the following scenarios: \textbf{direct} or \textbf{gateway} based interconnection (Sect. \ref*{subs.architecture}).

By design, the \textbf{direct} approach requires that the cloud has to be compliant with the interoperability standard the CPS is built upon - it becomes a consistent part of the CPS. Data models, roles, and responsibility differences of both solutions make this approach impractical or even imposable to be applied in typical cases. A more detailed description is covered by the Sect.~\ref*{cloud-to-sensors-field-level-connectivity}.

This article addresses further research on the integration of the multi-vendor \textbf{machine-centric} CPS designed atop of M2M communication and emerging cloud computing as a \textbf{human-centric} front-end in the context of the Industry 4.0 (I4.0) and Industrial Internet of Things (IIoT) disciplines. For this integration, a new architecture is proposed to support the reactive relationship of communicating parties. To support the multi-vendor environment OPC Unified Architecture \cite{LiteratureSurveyOnOpenPlatformCommunications} interoperability standard has been selected. Prototyping addresses Microsoft Azure Cloud as an example. The proposals are backed by proof of concept reference implementations - the outcome has been just published on GitHub as the open-source (MIT licensed) \todo{Add reference}. The proposed solutions have been harmonized with the more general concept called the Object-Oriented Internet \cite{RefWorks:doc:5c66740ae4b081adf5804596}.

The main goal of this article is to provide proof that:

\begin{enumerate}
      \item  reactive interoperability M2M communication based on the OPC UA standard can be implemented as a powerful standalone library without dependency on the Client/Server session-oriented archetype
      \item Azure interoperability can be implemented as an external part employing out-of-band communication without dependency on the OPC UA implementation
      \item the proposed generic architecture allows that the gateway functionality is implemented as composable part at run-time part - no programming required
\end{enumerate}

The remainder of this paper is structured as follows. Sect.~\ref*{cloud-to-sensors-field-level-connectivity} presents the proposed open and reusable software model. It promotes a reactive interoperability pattern and a generic approach to establishing interoperability-context. A reference implementation of this archetype is described in Sect.~\ref*{sect.gateway-implementation}. The most important findings and future work are summarized in Sect.~\ref*{section.conclusion}.

\section{Sensors to Cloud Field Level Connectivity}\label{cloud-to-sensors-field-level-connectivity}

\subsection{Architecture}\label{subs.architecture}

As it was explained in Sect.~\ref*{introduction}, to follow the Industry 4.0 concept a hybrid environment integrating reactive Machine to Machine interconnection and interactive web-based user interface is required (Sect. \ref*{introduction}). The main challenge of the solution in concern is to design a generic but reusable architecture that addresses interoperability of these diverse interconnection scenarios ruled by different requirements, namely

\begin{enumerate}
      \item \textbf{machine-centric} machine to machine real-time mobile interoperability
      \item \textbf{human-centric} cloud-based front-end
\end{enumerate}

Interconnection of the reactive \textbf{machine-centric} and interactive \textbf{human-centric} environments can be implemented by applying one of the following scenarios:

\begin{itemize}
      \item \textbf{direct interconnection} (tightly coupled scenario) - cloud-based dedicated communication services are engaged to attach it to the CPS making up a consistent M2M communication network using a common protocol stack
      \item \textbf{gateway based interconnection} (loosely coupled scenario) - native build-in communication services allows attaching the cloud to the CPS using an out-of-bound protocol stack
\end{itemize}

In the solution in concern, the interconnection of assets is not enough hence their interoperability is expected. In this case, using the same communication stack must be recognized as only a necessary condition. To support interoperability common data understanding is required. By design, the direct approach requires that the cloud has to be compliant with the interoperability standard the CPS uses. As a result, it becomes a consistent communication node of the CPS. Additionally, to meet this requirement the cloud and CPS have to establish directly the same

\begin{itemize}
      \item semantic-context
      \item security-context
\end{itemize}

The possibility to establish a common semantic-context in the multi-vendor environment makes communication standardization especially important. In this case, it is required that the encoding of the messages payload exchanged over the network (Data Transfer Object - DTO) is standardized so that the payload can be factored on the data-gathering site and consumed on the ultimate destination data processing sites. Security between the data origin and ultimate data destination refers to the protection of messages (security-context) against malicious users. It is required that communicating parties are using the same cyber-security measures.

The decision to follow the \textbf{direct interconnection} scenario must be derived from an analysis of the capabilities of available services in concern. However, for the development strategy of this type of solutions this analysis can be done partially taking into account two features that can be considered invariable:

\begin{itemize}
      \item by design the cloud-based services must be virtual - they are used to handle many solutions at the same time
      \item by design the M2M communication is usually constrained by the real-time requirements
\end{itemize}

The virtualization of cloud services means that they must be very flexible to handle the attachment of new assets proactively (acting in advance) at run time. As a result, by design, the cloud services must be responsible to register and authenticate devices by exposing endpoints in the public network to allow the device to access a provisioning cloud service. It requires that a session over the Internet has to be established by the data holding asset at a preparation step.

To meet the requirements of real-time distributed control the CPS may use protocols applicable only to local computer networks (e.g.~multicast IP, Ethernet, TSN \footnote{Time-Sensitive Networking (TSN) Task Group \url{https://1.ieee802.org/tsn/}}, etc.). Because the cloud services support only protocols handling interconnection over the Internet the interaction with the cloud requires remote agents, i.e.~agents attached locally to the M2M network and implemented by applying one of the following archetypes:

\begin{itemize}
      \item \textbf{edge device} - remote cloud agent acting as an intermediary for nodes of the cyber-physical network
      \item \textbf{field level gateway} - a dedicated custom agent acting as an intermediary for nodes of the cyber-physical network
      \item \textbf{embedded gateway} - a software part composed into a selected node of the cyber-physical network
\end{itemize}

\textbf{Edge device} connects directly to the cloud services but acts as an intermediary for other devices called leaf devices. Additionally, it allows the selection of initial data processing and execution of them using local resources. The \textbf{edge device} may be located close to the leaf devices and attached to the cyber-physical network using protocols applicable only to local computer networks. In this scenario, it is possible to use a custom protocol stack to get connected to the \textbf{edge device} with the cloud and helps to save the bandwidth thanks to sending only the results of local processing. In this approach, the \textbf{edge device} is part of cloud vendor products and cannot be recognized as a generic solution that can be used to connect to other clouds at the same time.

The \textbf{field level gateway} is also build atop of the middleware\todo{add ref} concept. The only difference compared with the \textbf{edge device} is necessity to use officially supported by the cloud vendor services to get connected. In this scenario the process data may be transferred to many clouds at the same time provided that the gateway offers this functionality.

\begin{figure}
      \centering
      \includegraphics[width=8cm]{../.Media/StrategyDomainModel.png}
      \caption{Domain model of a generic interconnection concept}\label{figure1.StrategyDomainModel}
\end{figure}

Unlike the above described solutions, the \textbf{embedded gateway} is not derived from the middleware concept. A generic domain model for this interconnection is presented in the Fig.~\ref*{figure1.StrategyDomainModel}. Promoting separation of concern design principle, the gateway functionality should be implemented as a self-contained software part embedded in the \textit{Networking} service of the \textit{Cyber-physical\ node}. Main functionality of this component is to transfer selected data between \textit{Cyber-physical\ network} using \textit{Networking} services of an existing \textit{Cyber-physical\ node} and \textit{Cloud-based\ front-end} using officially supported by the cloud vendor interconnection services.

The \textbf{embedded gateway} archetype relaxes most of the issues described above: \textit{Cyber-physical\ network} real-time behavior, data encoding incompatibility, security-context differences to name only a few. The main goal of this article is to provide proof that the \textbf{embedded gateway} archetype implementation is possible based on a generic architecture that can be used as a foundation for the integration of the heterogenous environments in concern. The proposed implementation is designed for selected interoperability standard and cloud product.

To comply with the Industry 4.0 communication criterion it is required that any product must be addressable over the network via TCP/UDP or IP and has to support the OPC UA Information Model. As a result, any product being advertised as Industry 4.0 enabled must be OPC UA-capable somehow. To support the multi-vendor environment OPC Unified Architecture interoperability standard has been selected. OPC UA supports the following two patterns to be used to transfer data between communicating parties:

\begin{itemize}
      \tightlist
      \item \textbf{session-oriented}: requires a session that has to be established before any data can be sent between sender and receiver
      \item \textbf{sessionless-oriented}: the sender may start sending messages (called packets or datagrams) to the destination without any preceding handshake procedure
\end{itemize}

Using the session-oriented communication pattern it is difficult or even impossible to gather and process mobile data (Sec.~\ref*{introduction} ), which is one of the Internet of Things paradigms. OPC UA Part 14 PubSub \cite{RefWorks:doc:5d98837de4b055984c0eecf0, UAPart14PubSubMainTechnologyFeatures} offers the sessionless approach as an additional option to session based client-server interoperability relationship and is a consistent part of the OPC UA specifications suit. As the result it can be recognized as the IoT ready technology.

The presented proposals in the article are backed by proof of concept reference implementations\cite{RefWorks:doc:5c66740ae4b081adf5804596} \todo{ref to MP, PS gateway implementation}. For this study, prototyping addresses Microsoft Azure cloud products. There are many reasons for selecting Azure to accomplish the cloud-based front-end of Cyber-Physical System (CPS). Azure offers Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) capabilities. As a result, the platform can be used not only as a cloud-based front-end for CPS. By design, the Azure services are compliant with Security Development Lifecycle (SDL) an industry-leading security process. It is also compliant with the new international standard for cloud privacy, namely ISO 27018. Solutions hosted on Azure are scaled up to millions of users without any additional coding. For the development of the todo front-end, it is essential that Azure provides very efficient storage services usefully for the purpose of real-time process data archival. Azure provides a vast variety of hybrid connections including but not limited to virtual private networks (VPNs), caches, content delivery networks (CDNs), ExpressRoute, and IoT dedicated services that can be directly used to implement cloud-based front-end for CPS. Because it is also integrated with other Microsoft tools like Office 365, Outlook, and SharePoint using Azure allows preserving investment and exporting process data to the mentioned tools. Azure also offers services supporting analytics and intelligence capabilities for further improving business processes and decision making. It is the only cloud platform that offers Blockchain as a Service (BaaS), Machine Learning, Bots, and Cognitive APIs capabilities.

Azure aids Internet protocols and open standards such as JSON, XML, SOAP, REST, MQTT, AMQP, and HTTP. A software development kits for C\#, Java, PHP, and Ruby are available for custom applications. Azure provides services supporting data exchange over the OPC UA, but they don't support PubSub compliant with the OPC UA Part-14. Connectivity services on the network use JSON-based Data Transfer Object encoded based on schema derived from the solution metadata.

More detailed description of the selected Azure features in context of the application in concern are covered by the Sec.~\ref*{azure-main-technology-features}.

Based on the sessionless and session-oriented communication patterns examination against the IoT requirements \cite{mpostol2020} it could be concluded that the connectionless pattern better suites issues related to the assets mobility and traffic asymmetry that is characteristic for the application domains in concern. Additionally, to promote interoperability and address the demands of the M2M communication in the context of a multi-vendor environment the prototyping should use a framework that must be compliant with the OPC UA Part 14 PubSub  specification. According to proposed generic architecture (Fig.~\ref*{figure1.StrategyDomainModel}) to implement the \textit{Embedded\ Gateway} as a composable part of the \textit{Cyber-physical\ node} a library implementing \textit{Networking} functionality in compliance with mentioned above specification is a starting point for further development. Additionally, it must be assumed that the library used to deploy \textit{Embedded\ Gateway} support dependency injection and be capable to compose an external part supporting Azure/PubSub gateway functionality. The composition process must be available without modification of the core code of an existing library. As a result, the prototyping is to be limited to implementation of the \textit{Embedded\ Gateway} software part only.

A library that meets all these requirements has been implemented consistently with the Object-Oriented Internet paradigm \cite{RefWorks:doc:5c66740ae4b081adf5804596} worked out in an open-source project \footnote{ \url{https://github.com/mpostol/OPC-UA-OOI} }. The paper \cite{mpostol2020} covers the description of a reference application program implementation proving that it is possible to design universal architecture targeting reactive interoperability as a consistent part of the Object-Oriented Internet concept compliant with the OPC UA PubSub international standard. According to the presented implementation and evaluation, using the dependency injection and late binding, the application program can be seamlessly adapted to the production environment and scales well. This approach also improves flexibility and adaptability of the existing solutions against any modification of the production environment including but not limited to the selected interoperability standard change.

Sect.~\ref*{ooi-main-technology-features} provides a more detailed description of this library and an external part deployment process that is to be used to implement new functionality supporting \textit{Embedded\ Gateway}.

The following subsections cover the description of the current state of technologies concerning Azure cloud-based IoT enabler and OPC UA PubSub. Sect.~\ref*{azure-main-technology-features} analyzes data presentation user interface, available native communication services, and data/metadata model offered by Microsoft Azure. The discussion covered by this section is the foundation for selecting services utilized to expose process data and suitable protocol stack to support interconnection. In Sect.~\ref*{ooi-main-technology-features} the discussion focuses on the generic architecture that is to be used as a foundation for further decisions addressing the systematic design of the interoperability of the CPS and cloud-based front-end.

\subsection{Azure Main Technology Features}\label{azure-main-technology-features}

\subsubsection{Services}\label{services}

Deployment of the hybrid solution providing interoperability of the \textbf{machine-centric} Cyber-Physical Systems (CPS) designed atop of M2M reactive communication and emerging cloud computing as a \textbf{human-centric} front-end requires decisions addressing the selection of the services supporting web user interface capable to expose process data. In this context, the service is any autonomous (with own identity) software component or module that is interfacing with selected CPS for data collection, analysis, and also remote control. Microsoft Azure is a cloud-based product. It offers a vast variety of services. This virtual environment handles an unlimited number of users and devices organized using a solution container concept. The solution aggregates users, devices, services, and required additional resources scoping on a selected scenario. It also serves as a context that provides a scope to the identifiers (the names of devices, users, process data entities, etc) inside it. Solutions are used to organize deployment entities into logical groups and prevent identity collisions.

The \textbf{IoT\ Central} service provides a process data visualization
user interface. To make this interface meaningful metadata called device
template is used to describe devices.

Following the assumption that interconnection between the CPS and cloud services is designed based on the gateway concept, a middleware must be considered as a coupler. It must be interconnected with the CPS using an in-band protocol adhering to communications requirements (i.e.~protocol profile, data encoding, time relationships, etc.) governing communication of the parts making it up. At the same time, it must support back-and-forth data transfer to the cloud using out-of-band native for the cloud services. The transfer process requires data conversion from source to destination encoding. The \textit{IoT\ Hub} is a service hosted in the cloud that supports \textit{IoT\ Central} providing a robust messaging solution - it acts as a central message hub for bi-directional communication \todo{Reference}. This communication is transparent, i.e.~it is not data types aware allowing
any devices to exchange any kind of data. This service is responsible to manage the devices' identity and it offers the following protocol stacks: AMQP, MQTT and HTTPS.

Before process data can be exposed using a web user interface the data source must be associated with an appropriate solution and validated to make sure that the security rules are not violated. It is hard to assume that the security rules governing the CPS may also apply to the cloud-based services. In the gateway scenario, they can be mapped on each other or be entirely independent. The \textit{IoT\ Hub\ Device\ Provisioning\ Service} (DPS) is a helper service for \textit{IoT\ Hub} that enables devices' connection process management, upon device providing valid identity attestation it assigns the device to an appropriate \textit{IoT\ Hub} instance and returns to the device connection parameters, which allow direct connection with given \textit{IoT\ Hub} service. The device proceeds to use the same attestation in \textit{IoT\ Hub} connection and based on it, is granted authorization to selected operations including but not limited to data transfer and updating the user interface.

It is worth stressing that interaction of the offered by the Azure
services can be configured flexibly, and as a result, the presented
above selection of services must be recognized as an example only. The
\textbf{IoT\ Central} can be also seamlessly integrated with other
services as needed. The following services could also be considered to
build cloud-based automation solution:

\begin{itemize}
      \item \textit{Industrial\ IoT} - discovering OPC UA enabled servers in a factory network and register them in Azure \textit{IoT\ Hub} implemented using \textit{IoT\ Edge}
      \item \textit{Digital\ Twins} - managing the graph of digital twins, which are to represent some real-world process or entity
\end{itemize}

\textit{Industrial\ IoT} promotes OPC UA client/server archetype used to
achieve direct and interactive interoperability implemented using
\textit{IoT\ Edge} services that allow extracting initial data
processing to local premises based on the edge concept.
\textit{Digital\ Twins} is an emerging concept to use an observer to
replicate selected process state and behavior. The possibility to add
value as a result of using these services must be subject to further
research.

\subsubsection{Data Interchange}\label{data-interchange}

System components interoperability means the necessity of the
information exchange between them. The main challenge of
interoperability implementation is that information is abstract -- it is
knowledge describing the process in concern state and behavior,
e.g.~temperature in a boiler, a car speed, an account balance, etc.
Obviously, abstraction cannot be processed by the cyber-physical
machines. It is also impossible to transfer abstraction from one place
to another over the network.

Fortunately, computer science offers a workaround to address that impossibility - the information must be represented as a binary stream. In consequence, we can usually use both terms as interchangeable ones while talking about ICT systems. Unfortunately, these terms must be precisely observed in the context of further discussion, because we must be aware of the fact that the same information could have many different but equivalent representations. In other words, the same information can be represented by a vast variety of different binary patterns. For example, numbers may be represented using 2's Complement and Floating-Point binary representations.

It should be nothing new for us, as it is obvious that the same
information printed as a text in regional newspapers in English, German,
Polish, etc. does not resemble one another, but the text meaning should
always be the same. To understand a newspaper we must learn the
appropriate language. To understand the binary data we must have defined
a data type -- a description of how to create an appropriate bits
pattern (syntax) and rules needed to assign the information (semantics),
i.e.~make any correct bitstream meaningful. Concluding, to make two
systems interoperable, a semantic-context must be established. The type
plays the role of metadata, a set of data that describes other data.
Metadata term is frequently used if the semantic-context is defined
using a native language to select built-in types engaging a
general-purpose graphical user interface.

Using the data type definition to describe information interchange between communicating parties allows:

\begin{itemize}
      \item Development against a type definition of the user interface
      \item Implementation of the functionality of the bitstreams conversion in advance
\end{itemize}

Having defined types in advance, a gateway may provide dedicated conversions functionality, i.e.~replacing bitstreams used by the CPS by equivalent ones for the cloud-based services. The Azure offers a vast variety of built-in types ready to be used in common cases, but not necessarily there are equivalent counterparts in use by the CPS. Additionally, the data conversion must address the following issues:

\begin{itemize}
      \item usually to covert data from source to destination representation, the middleware software native types must be used
      \item if the out of the box set of types is not capable of fulfilling more demanding needs, custom data types must be defined
\end{itemize}

Although the data conversion is a run-time gateway task the
implementation of the conversion algorithms must be recognized as an
engineering task, and therefore this topic is not considered for further
discussion.

In \textbf{IoT\ Central} a CPS is represented as a set
of devices. The characteristics and behavior of each device kind are
described by the device template. This Device Template (DT) contains
also metadata describing the data exchanged over the
wire with the CPS called Device Capability Model
(DCM). Additionally, the DT contains properties, customization, and
views definitions used by the service locally. As an option, DCM
expressed as a JSON-LD can be imported into a Device Template.
\textbf{IoT\ Central} allows also to create and edit a DCM using the
dedicated web UI. A JSON file containing DCM can be derived from an
information model used as a foundation to establish the semantic-context
applied to achieve interoperability of the devices interconnected as the
CPS. DCM development against any external information
model is a design-time task and should be supported by dedicated
development tools. In any case, the data interchanged between the cloud
and the gateway must be compliant with the DCM, hence the middleware
software must be aware of conversions that must be applied to achieve
this interoperability.

\subsubsection{Connectivity}\label{subsubsection.connectivity}

From the cloud side, it is proposed to employ the \textit{IoT\ Hub} service to handle the network traffic targeting the cyber-physical system. This service offers profiles of the AMQP, MQTT, and HTTPS protocol stacks. In any case, process data is transparently transferred back-and-forth to the upper layer \textbf{IoT\ Central} service. Hence, the payload formatting is determined by the DCM associated with the \textbf{IoT\ Central} solution.

The Azure supports HTTP, AMQP, and MQTT protocol stacks which are all standard ones. Consequently, it is possible to apply any available ones compliant with an appropriate specification to achieve connectivity. In this case, all parameters required to establish semantic and security contexts are up to the gateway responsibility. Alternatively, the API offered by the dedicated frameworks (libraries) may be used. Using a framework the configuration process may be reduced significantly, and the communication protocol selection has only an indirect impact on the interoperability features. The Azure interconnection may be obtained using the following \textit{Microsoft.Azure} packages:

\begin{itemize}
      \item \textit{Devices} - Service SDK for Azure IoT Devices
      \item \textit{Devices.Client} - Device SDK for Azure \textit{IoT\ Hub} including dependencies implementing MQTT and AMQP communication
      \item \textit{Devices.Shared} - Common code for Azure IoT Device and Service SDK
      \item \textit{Devices.Provisioning.Client} - Provisioning Device Client SDK for Azure IoT Devices
      \item \textit{Devices.Provisioning.Transport.*} - Provisioning Device Client Transports (HTTPS, AMQP, MQTT) for Azure IoT Devices
\end{itemize}

\subsection{OOI Main Technology Features}\label{ooi-main-technology-features}

To promote interoperability and address the demands of the M2M communication in the context of a multi-vendor environment the prototyping should use a framework that must be compliant with the OPC UA Part 14 PubSub (Sect. \ref*{subs.architecture}) and support the \emph{Reactive Interoperability} (Sect. \ref*{introduction}) concept. A framework compliant with these requirements has been implemented as an open-source library named \textit{UAOOI.Networking} (\textit{Networking} for short) under an umbrella of the project Object-Oriented Internet \cite{RefWorks:doc:5c66740ae4b081adf5804596}. The library is designed to be a foundation for developing application programs that are taking part in a message-centric communication pattern and interconnected using the reactive networking concept. The diagram in Fig.~\ref*{figure2.UADataApplicationArchitecture} shows the relationship between the library (\textit{SDK}) and external parts composing any reactive networking application (\textit{Reactive\ Application}). The \textit{Reactive Application} is an aggregation of parts implementing the \textit{Producer} and \textit{Consumer} roles. By design, they support access to real-time process data, hence they are recognized as an extension of \textit{DataRepository} class. To implement the \textit{DataRepository} dedicated implementation of the \textit{IBinding} interface should be provided to create a bridge between CPS and an external row data represented by the \textit{LocalResources} class. A more in-depth description of the \textit{OOI Reactive Application} library enabling data exchange over a network using the reactive networking pattern is in \cite{mpostol2020}.

\begin{figure}
      \centering
      \includegraphics[width=8cm]{../.Media/UADataApplicationArchitecture.png}
      \caption{Reactive interoperability implementation}\label{figure2.UADataApplicationArchitecture}
\end{figure}

From the above discussion, we can learn that the main design decisions must concern standardization and flexibility. Standardization needs the selection of an international interoperability specification to make the library ready to be adopted by the multi-vendor environment. Flexibility requires an architecture that promotes the polymorphic independent implementation of essential functions.

Piece by piece integration of a cyber-physical system using multi-vendor products requires that M2M communication employs international standards as the interoperability foundation. OPC Unified Architecture Part 14 PubSub has been selected in this respect (Sect. \ref*{subs.architecture}). By design, this standard should support the required publisher-subscriber communication pattern. It must be stressed, that by design, the specification provides only an abstract interoperability definition. Abstract means that the standard must not limit the implementation strategy.

Many parts in the domain model presented in Fig.~\ref*{figure2.UADataApplicationArchitecture} requires a polymorphic approach to implementation. To promote the polymorphic ready solution the following concepts have been adopted:

\begin{itemize}
      \item \textbf{separation of concerns} - to allow an independent development of the parts~\cite{RefWorks:doc:5d92609be4b02eb43d372bd1}
      \item \textbf{dependency injection} - to allow late binding of separately implemented parts~\cite{RefWorks:doc:5d925b77e4b030b4e0596f5d}
\end{itemize}

To promote the polymorphic approach, the library has a factory class called \emph{DataManagementSetup} that is a placeholder to gather all injection points used to compose external parts. To be injected the parts must be compliant with an appropriate contract expressed as the following interfaces:

\begin{itemize}
      \item \emph{IBindingFactory} - bidirectional data exchange with the underlying process
      \item \emph{IConfigurationDataFactory} - the configuration data access
      \item \emph{IMessageHandlerFactory} - \emph{Message} entities pushing to/pulling from the wire
      \item \emph{IEncodingFactory} - searching a dictionary containing value converters
\end{itemize}

It is expected that the functionality implementation expressed by these interfaces is provided as independent external composable parts. The composition is accomplished at run time, and the effective application functionality depends essentially on reusable loosely coupled parts composed applying the dependency injection software engineering and adaptive programming concept.

\todo{ cite{RefWorks:doc:5d9796cbe4b0f66c52dccf04} }

\todo{convert textit => emph}

The \textit{DataRepository} represents data holding assets in the \textit{Reactive Application} implementing the \textit{IBindingFactory} interface. It captures functionality responsible for accessing the external process data from \textit{LocalResources}. The \textit{LocalResources} represents an external part that has a very broad usage purpose. For example, it may be any kind of the process data source/destination, i.e. raw data (e.g. PLC internal registers), OPC UA Address Space Management, cloud, file, database, graphical user interface, to name only a few.

Depending on the expected network role the library supports the implementation of:

\begin{itemize}
      \item \textit{Consumer} - entities processing data from incoming messages,
      \item \textit{Producer} - entities gathering process data and populating outgoing messages.
\end{itemize}

The \textit{Consumer} and \textit{Producer} classes are derived from the \textit{DataRepository} (Fig.~\ref*{figure2.UADataApplicationArchitecture}). The \textit{Consumer} uses the \textit{IBindingFactory} to gather the data recovered from the \emph{Message} instances pulled from a network. The received data may be processed or driven to any data destination, e.g. cloud-based front-end. The \textit{Producer} mirrors the \textit{Consumer} functionality and, after reading data from an associated source, populates the \emph{Message} using the gathered data.

By design, the \textit{DataRepository} and associated entities, i.e.~\textit{Local Resources}, \textit{Consumer}, \textit{Producer} are embedded in external parts, and consequently, the application scope may cover practically any concern that can be separated from the core \textit{Reactive Application} implementation.

The application startup phase concerns all the parts composed to make a running instance of the application program using the dependency injection approach to allow separate development and late binding. This approach makes any modification straightforward at any development and maintenance stage. From the end-user point of view, the composition process of the running program using independently developed parts is invisible. Because in any case a single program instance is created in a typical approach, we shall expect a commonly shared configuration mechanism providing mutually exclusive parameters to separately developed parts. This scenario has been addressed by the proposed implementation thanks to making the configuration expandable.

From the discussion covered by the Sect. \ref*{subs.architecture} we can infer that the interoperability of communicating parties requires establishing a semantic-context and security-context between them. If the received data is driven to any external data destination (e.g. cloud-based front-end) using out-of-band protocol semantic-context and security-context must be defined for this interoperability relationship. There are a vast variety of possibilities for how to define these contexts. Anyway, the most common starting point is the configuration. The proposed approach to instantiate the application as a set of composable parts is very useful in this respect because it makes it possible to expand the core configuration functionality and adapt it to be helpful also in this case. Therefore, the semantic-context may be created based on the configuration using the implementation of the \textit{IConfigurationDataFactory} interface.

Concluding, decoupling of this functionality implementation using an abstract contract and late binding mechanism allows:

\begin{itemize}
      \item implementation of practically any configuration management scenario,
      \item modification of this functionality later after releasing the library or deploying the application program in the production environment.
\end{itemize}

\section{Azure - Object-Oriented Internet Interoperability Implementation}\label{sect.gateway-implementation}

\subsection{Implementation Architecture}\label{subsection.GatewayImplementationArchitecture}

A generic domain model presenting interconnection architecture between the Azure \textit{Cloud-based\ Front-end} and \textit{Cyber-physical\ node} attached to the \textit{Cyber-physical\ network} is presented in Fig. \ref*{figure1.StrategyDomainModel}. It is proposed to implement the \textit{Cyber-physical\ node} by adopting the \textit{Reactive Application} archetype compliant with the reactive interoperability concept (Fig. \ref*{figure2.UADataApplicationArchitecture}) described in the Sect. \ref*{ooi-main-technology-features}. Merging selected entities of this archetype into the proposed domain model (Fig. \ref*{figure1.StrategyDomainModel}) lead to a model expressed as the diagram presented in Fig. \ref*{figure3.ImplementationDomainModel}. In the proposed approach the \textit{Embedded Gateway} is derived from the \textit{Consumer} role implemented as as a composable part aggregated by the \textit{Reactive Application}.

\begin{figure}
      \centering
      \includegraphics[width=8cm]{../.Media/ImplementationDomainModel.png}
      \caption{Architecture Domain Model}\label{figure3.ImplementationDomainModel}
\end{figure}

\hyphenation{PartData-Mana-gement-Setup}

In the final deployment architecture (Fig. \ref*{figure4.ImplementationArchitecture}) the \textit{Consumer} role has been realized by the \textit{PartDataManagementSetup} that is derived from \textit{DataManagementSetup} provided by the library. \textit{Networking} (SDK) was removed from this diagram for the sake of simplicity. Instantiating \textit{PartDataManagementSetup} is the first step for bootstrapping process of the \textit{Consumer} role functionality. This class provides an entry point to initialize all properties, which are injection points of all parts composing this role. It extends functionality of the \textit{DataManagementSetup} based on the following associated classes:

\todo{Overfull}

\begin{figure}
      \centering
      \includegraphics[width=10cm]{../.Media/ImplementationArchitecture.png}
      \caption{Implementation Architecture}\label{figure4.ImplementationArchitecture}
\end{figure}

\begin{enumerate}
      \item \textit{PartConfigurationFactory} - to get access to the application configuration
      \item \textit{PartBindingFactory} - to gather the data recovered from the \textit{Message} instances pulled from the wire
      \item \textit{CommunicationContext} - to handle communication with the Azure services
\end{enumerate}

In this case, the implementation of the \textit{IConfigurationFactory} interface is based entirely on the \textit{Networking} library. The implementation provided by the library is generic, but it is flexible enough to meet all the requirements, which warrants no need for further modifications for adoption. It is ready to provide all necessary configuration parameters to establish semantic and security contexts related to Azure interoperability.

The \textit{PartBindingFactory} implements the \textit{IBindingFactory} to gather the data recovered from the \emph{Message} instances pulled from a network. The received data is driven to \textit{CommunicationContext} for further encoding and finally pushing it to the Azure services using configured out-of-band protocol.

\textit{IEncodingFactory} and \textit{IMessageHandlerFactory} interfaces (Fig. \ref*{figure2.UADataApplicationArchitecture}) have been implemented in independent libraries and ae not considered for any modifications to deploy \textit{Embedded Gateway} functionality. The current implementation of the mentioned interfaces is localized at run time as services using an instance of the \textit{IServiceLocator}\footnote{ \url{https://www.nuget.org/packages/CommonServiceLocator} }. In this example, it is assumed that \textit{IServiceLocator} is implemented to resolve references to any external services. Both interfaces are used to manage interconnection of the node with the \textit{Cyber-physical Network}.

The Azure interconnection is realized using the \textit{CommunicationContext}. It implements message encoder and establishes the out-of-band communication stack.

The recovered data from the \textit{Message} is obtained from the \textit{PartBindingFactory} using the \textit{IDTOProvider} that defines a contract used to pull the Data Transfer Object (DTO) created from a selected subscription by the \textit{PartBindingFactory}. The transfer process requires data conversion from source to destination encoding, i.e. replacing bitstreams used by the CPS by equivalent ones for the cloud-based services. The Azure offers a vast variety of built-in types ready to be used in common cases, but not necessarily there are equivalent counterparts in use by the CPS. The Azure uses JSON based Data Transfer Object encoding and schema defined based on the solution metadata. The PubSub uses JSON and binary Data Transfer Object encodings. In any case the data recovered from the Message pulled from a subscription is stored locally using the object model based on standard .NET types. \textit{PartBindingFactory} maps selected object graph onto the JSON message required by the cloud services.

The encoded JSON messages must be transferred over the network to Azure using the selected protocol stack. The Azure supports standard protocols (Sec. \ref*{azure-main-technology-features}). Consequently, it is possible to apply any available ones compliant with an appropriate specification to achieve connectivity. In the proposed gateway implementation the Azure interconnection has been obtained using the packages described in Sect. \ref*{subsubsection.connectivity}.

Azure and PubSub uses different security mechanism so in the proposed solution establishing security-context is realized independently. The \textit{CommunicationContext} is responsible to establish this context as an embedded negotiation phase tightly coupled with establishing interconnection.

\section{Conclusion}\label{section.conclusion}

\todo{vendor products and cannot be recognized as a generic solution that can be used to connect to other clouds at the same time}

Nowadays, the macro optimization of the industrial processes requires an integration of a vast variety of distributed applications provided by the Information and Communication Technology. It requires further research on the integration of \textbf{machine-centric} Cyber-Physical Systems (CPS) with \textbf{human-centric} front-end in the context of new emerging disciplines, i.e. Industry 4.0 and the Industrial Internet of Things (Sect. \ref*{introduction}).

CPS is composed using the multi-vendor components (data holding assets) interconnected atop of the Machine To Machine (M2M) communication. In many applications, the dynamic nature of the CPS must be considered. Dynamic nature means that interconnected assets may be added/removed from the network at any time. By design, typically CPS must fulfill the real-time and mobility of the assets requirements.

Highly-distributed solutions used to control/monitor a set of geographically spread islands of automation (e.g. virtual power plants producing renewable energy) additionally must leverage public communication infrastructure, namely the Internet. If islands of automation must be controlled over the Internet, the cloud computing concept may be recognized as a reasonable answer. Following this concept, the cloud-based supervisory control functionality is applied as a set of services employing abstraction and virtualization - two main pillars of the cloud computing paradigm. In the cloud computing concept, virtualization is recognized as the possibility to share the services by many users, and abstraction hides implementation details.

The main goal of this research is working out a new generic architecture and deployment scenario applicable for the integration of the \textbf{machine-centric} CPS and emerging cloud computing as a \textbf{human-centric} front-end.

If we must bother with the network traffic propagation asymmetry or mobility of the asset network attachment-points the reactive relationship\cite{mpostol2020} could alleviate the challenges posed by the interactive approach. The real-time multi-vendor environment makes communication standardization especially important. To support this environment, OPC Unified Architecture \cite{RefWorks:doc:5ac86c98e4b009947bbb874c} interoperability standard has been selected. As it was pointed out in the Sect. \ref*{subs.architecture} using OPC UA PubSub \cite{RefWorks:doc:5d98837de4b055984c0eecf0} the aggregation of nodes by the network is loosely coupled, i.e. nodes can be added and removed from the network dynamically, and nodes may represent mobile data holding assets.

From the analysis covered by the Sect. \ref*{subs.architecture} it is concluded that the \textbf{embedded gateway} archetype best suits all requirements described above. It relaxes most of the issues related to \textbf{direct interconnection} and solutions inferred from the middleware concept, i.e. \textbf{edge device} - remote cloud agent and \textbf{field level gateway} - a dedicated custom agent. The generic domain model for the proposed interconnection archetype is presented in the Fig.~\ref*{figure1.StrategyDomainModel}. We have highlighted that to promote the separation of concern design principle, the gateway functionality should be implemented as a self-contained dedicated software part embedded in the core \textit{Networking} service of the \textit{Cyber-physical\ node}. Main functionality of this component is to transfer process data between \textit{Cyber-physical\ network} using \textit{Networking} services of an existing \textit{Cyber-physical\ node} and \textit{Cloud-based\ front-end} using officially supported by the cloud vendor interconnection services.

To promote interoperability and address the demands of the M2M communication in the context of a multi-vendor environment the prototyping should use a framework that must be compliant with the OPC UA Part 14 PubSub (Sect. \ref*{subs.architecture}) and support the \emph{Reactive Interoperability} (Sect. \ref*{introduction}) concept. We proposed to use an open-source library named \emph{UAOOI.Networking} (\emph{Networking} for short) (Fig.~\ref*{figure2.UADataApplicationArchitecture}) for this purpose. It is worth stressing that based on this approach only dedicated functionality related to the communication with the cloud must be implemented.

We derived the final model presented in Fig. \ref*{figure4.ImplementationArchitecture} by merging selected entities from the \emph{Networking} library (Fig.~\ref*{figure2.UADataApplicationArchitecture}) into the generic interconnection domain model (Fig. \ref*{figure1.StrategyDomainModel}). In the proposed approach the \textit{Embedded Gateway} is derived from the \textit{Consumer} role implemented as as a composable part aggregated by the \textit{Reactive Application}.

The proposals are backed by proof of concept reference implementations. Prototyping addresses Microsoft Azure cloud as an example. The outcome has been just published on GitHub as the open-source (MIT licensed) repository. The proposed solutions have been harmonized with the more general concept called the Object-Oriented Internet.

The described results prove that the \textbf{embedded gateway} archetype implementation is possible based on the existing standalone framework supporting reactive interoperability atop of the M2M communication compliant with the OPC UA PubSub standard. It is worth stressing that there is no dependency on the Client/Server session-oriented relationship. This relationship is in contrast to the architecture described in the OPC UA Part 1 \cite{OPCUAPart1} specification where the publisher role is tightly coupled with the \textbf{Address Space} embedded component of the OPC UA Server. In the proposed approach the cloud interoperability is obtained by implementing a dedicated part employing out-of-band communication only without dependency on the OPC UA functionality at all. It is worth stressing that the gateway functionality is implemented as a part - composable to the whole without programming skills. Because the part is composed at the runtime it makes it possible to modify its functionality later after releasing the library or even deploying the application program in the production environment.

Concluding, the paper describes a proof of the concept that applying the proposed architecture and deployment scenario it is possible to integrate selected cloud services (e.g. \textbf{Azure\ IoT\ Central}) with the \emph{Cyber-physical network} interconnected as one whole atop of the OPC UA PubSub. It is in contrast to interconnecting cloud-based front-end services with the selected \emph{ Address\ Space} exposed by a selected OPC UA server limiting the PubSub role to data exporter transferring the data out of the OPC UA ecosystem.

\bibliography{Manuscript.bib}
\bibliographystyle{aiaa-jacs.bst}

\end{document}
